{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8993587,"sourceType":"datasetVersion","datasetId":5417141}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-19T20:07:51.197802Z","iopub.execute_input":"2024-07-19T20:07:51.198341Z","iopub.status.idle":"2024-07-19T20:07:52.380189Z","shell.execute_reply.started":"2024-07-19T20:07:51.198294Z","shell.execute_reply":"2024-07-19T20:07:52.378880Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/titanic/train.csv\n/kaggle/input/titanic/test.csv\n/kaggle/input/titanic/gender_submission.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# A intensão desse teste é ver se consigo me sair melhor que o chatgpt no desafio do Titanic.\n#### O prompt no chatgpt pediu apenas para criar uma forma de fazer comparacoes entre modelos.","metadata":{}},{"cell_type":"code","source":"gs = pd.read_csv('/kaggle/input/titanic/gender_submission.csv')\ntest = pd.read_csv('/kaggle/input/titanic/test.csv')\ntrain = pd.read_csv('/kaggle/input/titanic/train.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:09:34.948406Z","iopub.execute_input":"2024-07-19T20:09:34.948886Z","iopub.status.idle":"2024-07-19T20:09:34.974102Z","shell.execute_reply.started":"2024-07-19T20:09:34.948836Z","shell.execute_reply":"2024-07-19T20:09:34.972792Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"gs.info()\ntest.info()\ntrain.info()\n","metadata":{"execution":{"iopub.status.busy":"2024-07-19T20:13:38.194587Z","iopub.execute_input":"2024-07-19T20:13:38.195283Z","iopub.status.idle":"2024-07-19T20:13:38.216772Z","shell.execute_reply.started":"2024-07-19T20:13:38.195249Z","shell.execute_reply":"2024-07-19T20:13:38.215517Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 2 columns):\n #   Column       Non-Null Count  Dtype\n---  ------       --------------  -----\n 0   PassengerId  418 non-null    int64\n 1   Survived     418 non-null    int64\ndtypes: int64(2)\nmemory usage: 6.7 KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 418 entries, 0 to 417\nData columns (total 11 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  418 non-null    int64  \n 1   Pclass       418 non-null    int64  \n 2   Name         418 non-null    object \n 3   Sex          418 non-null    object \n 4   Age          332 non-null    float64\n 5   SibSp        418 non-null    int64  \n 6   Parch        418 non-null    int64  \n 7   Ticket       418 non-null    object \n 8   Fare         417 non-null    float64\n 9   Cabin        91 non-null     object \n 10  Embarked     418 non-null    object \ndtypes: float64(2), int64(4), object(5)\nmemory usage: 36.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Teste chatgpt Scikit-learn (é necessario adaptar)","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Carregar um conjunto de dados de exemplo\nfrom sklearn.datasets import load_iris\ndata = load_iris()\nX = data.data\ny = data.target\n\n# Dividir os dados em conjunto de treino e teste\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models = {\n    \"Logistic Regression\": LogisticRegression(),\n    \"Random Forest\": RandomForestClassifier(),\n    \"SVM\": SVC(probability=True)\n}\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = {}\nfor model_name, model in models.items():\n    # Treinar o modelo\n    model.fit(X_train, y_train)\n    \n    # Fazer predições\n    y_pred = model.predict(X_test)\n    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n    \n    # Avaliar o modelo\n    accuracy = accuracy_score(y_test, y_pred)\n    f1 = f1_score(y_test, y_pred, average='macro')\n    roc_auc = roc_auc_score(y_test, y_proba, multi_class='ovo') if y_proba is not None else None\n    \n    # Armazenar os resultados\n    results[model_name] = {\n        \"accuracy\": accuracy,\n        \"f1_score\": f1,\n        \"roc_auc\": roc_auc\n    }\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_df = pd.DataFrame(results).T\nprint(results_df)","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Teste chatgpt Yellowbrick, mlflow e Hyperopt (é necessario adaptar)","metadata":{}},{"cell_type":"code","source":"\"\"\"Yellowbrick: É uma biblioteca visual de machine learning que estende o Scikit-learn \ne permite a criação de visualizações de diagnóstico dos modelos.\n\n\"\"\"\n\nfrom yellowbrick.classifier import ClassificationReport\nvisualizer = ClassificationReport(RandomForestClassifier())\nvisualizer.fit(X_train, y_train)\nvisualizer.score(X_test, y_test)\nvisualizer.show()","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"MLflow: É uma ferramenta de gerenciamento de ciclo de vida de machine learning\nque pode ser usada para rastrear e comparar experimentos.\n\"\"\"\n\nimport mlflow\nimport mlflow.sklearn\n\nwith mlflow.start_run():\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    mlflow.log_metric(\"accuracy\", accuracy)\n    mlflow.sklearn.log_model(model, \"model\")\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"Hyperopt: Focada na otimização de hiperparâmetros, \nesta biblioteca pode ser usada em conjunto com Scikit-learn \npara encontrar e comparar os melhores modelos.\n\"\"\"\n\nfrom hyperopt import fmin, tpe, hp, Trials\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(params):\n    clf = RandomForestClassifier(**params)\n    return -cross_val_score(clf, X_train, y_train, cv=5).mean()\n\nspace = {\n    'max_depth': hp.choice('max_depth', range(1, 20)),\n    'n_estimators': hp.choice('n_estimators', range(10, 100))\n}\n\nbest = fmin(objective, space, algo=tpe.suggest, max_evals=100)\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Projeto pessoal usando consulta a livros.\n* Python para analise de dados\n* Data science do zero\n* Machine learning\n* Mãos a obra: Aprendizado de máquina com Scikit-learn & TensorFlow","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}